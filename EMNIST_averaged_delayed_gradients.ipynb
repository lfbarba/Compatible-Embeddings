{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EMNIST_averaged_delayed_gradients.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfbarba/Compatible-Embeddings/blob/master/EMNIST_averaged_delayed_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZm3k_JwAxFK",
        "outputId": "0bf68eb5-7ca9-41bd-8b0b-a44f7daf3e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip3 install torch torchvision\n",
        "\n",
        "!pip3 install --upgrade wandb\n",
        "!wandb login d6f99b98acf9c1a284aa2ba5830f3eca60fde2f0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.6/dist-packages (0.8.20)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (4.0.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: argh>=0.24.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.26.2)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied, skipping upgrade: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oX6ISEIaA5k"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PKcBS6xfBK1Z",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import math \n",
        "import gc\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "# Init wandb\n",
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEHBBRoVIyf",
        "colab_type": "code",
        "outputId": "9a90a617-d459-4827-af92-d126cf48c253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hC4ZVkTRB79P",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((28,28)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,), (0.5,))\n",
        "                               ])\n",
        "training_dataset = datasets.EMNIST(root='', split = 'byclass', train=True, download=False, transform=transform)\n",
        "validation_dataset = datasets.EMNIST(root='', split = 'byclass', train=False, download=False, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCqX-QWODId3",
        "colab": {}
      },
      "source": [
        "def im_convert(tensor):\n",
        "  image = tensor.cpu().clone().detach().numpy()\n",
        "  image = image.transpose(1, 2, 0)\n",
        "  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "  image = image.clip(0, 1)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFu3--lIeGMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(CNN, self).__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if dataset == \"cifar10\" or dataset == \"mnist\" or dataset.startswith(\"emnist\"):\n",
        "            if dataset == \"cifar10\" or dataset == \"mnist\":\n",
        "                self.num_classes = 10\n",
        "            elif dataset == 'emnist-byclass':\n",
        "                self.num_classes = 62\n",
        "            elif dataset == 'emnist-balanced' or dataset == 'emnist-bymerge':\n",
        "                self.num_classes = 47\n",
        "\n",
        "            if dataset == \"cifar10\":\n",
        "                side_size = 32\n",
        "                n_filters = 3\n",
        "            else:\n",
        "                n_filters = 1\n",
        "                side_size = 28\n",
        "\n",
        "            flatten_side_size = side_size - 2 * 3\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(n_filters, 32, (3, 3)),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(32, 64, (3, 3)),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, (3, 3)),\n",
        "                nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(64 * flatten_side_size * flatten_side_size, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, self.num_classes)\n",
        "            )\n",
        "\n",
        "        elif dataset.startswith(\"digits\"):\n",
        "            if dataset == \"digits\":\n",
        "                self.num_classes = 10\n",
        "            else:\n",
        "                self.num_classes = int(dataset[6:])\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(1, 8, (3, 3)),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(8, 16, (3, 3)),\n",
        "                nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(16 * 4 * 4, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, self.num_classes)\n",
        "            )\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w6wxPBg_Od3t",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "  def __init__(self, splitter, validation_loader, num_workers):\n",
        "    self.num_workers = num_workers\n",
        "    self.splitter = splitter\n",
        "    self.validation_loader = validation_loader\n",
        "\n",
        "  def setUpExperiment(self, model, device, config):\n",
        "    self.config = config\n",
        "    self.algorithm = self.config.algorithm\n",
        "    self.max_num_rounds = self.config.num_rounds_to_run\n",
        "    self.num_local_steps = self.config.num_local_steps\n",
        "    self.device = device\n",
        "    self.sampling_rate  = self.config.sampling_rate\n",
        "    self.with_training_loss = self.config.with_training_loss\n",
        "\n",
        "    self.training_iterators = []\n",
        "    for worker in range(0, self.num_workers):\n",
        "      self.training_iterators.append(self.getLoaderIter(worker))\n",
        "    \n",
        "    self.criterion = self.config.criterion\n",
        "    self.model = model\n",
        "\n",
        "    wandb.watch(self.model)\n",
        "\n",
        "    self.step_size = self.config.step_size\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.step_size)\n",
        "    # self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[2000, 4000], gamma=0.1)\n",
        "    \n",
        "    self.delta = []\n",
        "    self.sum_error = []\n",
        "    self.avg_error = []\n",
        "    param_list = list(self.model.parameters())\n",
        "    for i in range(0, len(param_list)):\n",
        "        self.delta.append(param_list[i]*0)\n",
        "        self.sum_error.append(param_list[i]*0)\n",
        "        self.avg_error.append(param_list[i]*0)\n",
        "\n",
        "    self.num_rounds = 0\n",
        "    \n",
        "\n",
        "    while self.num_rounds <= self.max_num_rounds:\n",
        "      if self.algorithm == 'FedLearn':    \n",
        "        self.runRoundFedLern()\n",
        "      elif self.algorithm == 'DAC':\n",
        "        self.runRound()\n",
        "      else:\n",
        "        print('No valid algorithm provided')\n",
        "        return\n",
        "      self.num_rounds += 1\n",
        "      if self.num_rounds % 2 == 0:\n",
        "        print('Num_local_steps : ', self.num_local_steps, ', rounds communication : ', self.num_rounds)\n",
        "        if self.with_training_loss:\n",
        "          #Chek Training data\n",
        "          avg_loss, avg_acc = self.checkModel('Training', self.splitter.full_training_loader)\n",
        "          wandb.log({\"Training Accuracy\": avg_acc, \"Training Loss\": avg_loss}, step = self.num_rounds)  \n",
        "        #Check Validation data\n",
        "        avg_loss, avg_acc = self.checkModel('Validation', self.validation_loader)\n",
        "        wandb.log({\"Test Accuracy\": avg_acc, \"Test Loss\": avg_loss}, step = self.num_rounds)\n",
        "        \n",
        "\n",
        "  def getLoaderIter(self, worker):\n",
        "    return iter(self.splitter.data_loaders[worker])\n",
        "\n",
        "  def getInputsLabels(self, training_list):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for worker in training_list:\n",
        "      try:\n",
        "        batch_inputs, batch_labels = self.training_iterators[worker].next()\n",
        "      except:\n",
        "        self.training_iterators[worker] = self.getLoaderIter(worker)\n",
        "        batch_inputs, batch_labels = self.training_iterators[worker].next()\n",
        "\n",
        "      inputs.append(batch_inputs)\n",
        "      labels.append(batch_labels)\n",
        "      \n",
        "    joined_inputs = torch.cat(inputs, 0).to(self.device)\n",
        "    joined_labels = torch.cat(labels, 0).to(self.device)\n",
        "    return joined_inputs, joined_labels\n",
        "\n",
        "  def computeJoinedGradient(self):\n",
        "\n",
        "    training_list = list(np.arange(0, self.config.num_workers))\n",
        "    random.shuffle(training_list)\n",
        "    training_list = training_list[0:math.ceil(self.config.sampling_rate * self.config.num_workers)]\n",
        "    \n",
        "    joined_inputs, joined_labels = self.getInputsLabels(training_list)\n",
        " \n",
        "    outputs = self.model(joined_inputs)\n",
        "    loss = self.criterion(outputs, joined_labels)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "  def runRoundFedLern(self):\n",
        "    # Run several batches on the same parameters and in the end apply the \n",
        "    # average of the computed gradients\n",
        "    for i, param in enumerate(self.model.parameters()):\n",
        "      self.sum_error[i] = self.sum_error[i] * 0\n",
        "\n",
        "    for current_local_step in range(0, self.num_local_steps):\n",
        "      #Computes the average joined gradient of all the workers\n",
        "      self.computeJoinedGradient()\n",
        "      for i, param in enumerate(self.model.parameters()):\n",
        "        self.sum_error[i] += param.grad\n",
        "        # This line is only useful in the last step when the gradient is set\n",
        "        # to be the average of the joined gradients \n",
        "        param.grad = self.sum_error[i]/self.num_local_steps\n",
        "\n",
        "    # After going through all the data, it gives just one step     \n",
        "    self.optimizer.step()  \n",
        "\n",
        "  def runRound(self):\n",
        "    # Update the error and sum before starting a new round\n",
        "    for i, param in enumerate(self.model.parameters()):\n",
        "      self.avg_error[i] = self.sum_error[i]/self.num_local_steps\n",
        "      self.delta[i] = self.delta[i] + self.avg_error[i]\n",
        "      self.sum_error[i] = self.sum_error[i] * 0\n",
        "\n",
        "    for current_local_step in range(0, self.num_local_steps):\n",
        "      self.computeJoinedGradient()\n",
        "      for i, param in enumerate(self.model.parameters()):\n",
        "        self.sum_error[i] += param.grad - self.delta[i]\n",
        "        param.grad = self.delta[i] + self.avg_error[i]\n",
        "\n",
        "      self.optimizer.step()  \n",
        "\n",
        "  def checkModel(self, data_label, loader):\n",
        "    sum_loss = 0.0\n",
        "    sum_corrects = 0.0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in loader:\n",
        "        batch_inputs = inputs.to(self.device)\n",
        "        batch_labels = labels.to(self.device)\n",
        "        outputs = self.model(batch_inputs)\n",
        "        loss = self.criterion(outputs, batch_labels)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        sum_loss += loss.item()\n",
        "        sum_corrects += torch.sum(preds == batch_labels.data)\n",
        "        \n",
        "    avg_loss = sum_loss/len(loader)\n",
        "    avg_acc = sum_corrects.float()/ len(loader)\n",
        "    print('{} loss: {:.4f}, {} acc {:.4f} '.format(data_label, avg_loss, data_label, avg_acc.item()))\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p71eiBISwTfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Partition(object):\n",
        "    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n",
        "\n",
        "    def __init__(self, data, index):\n",
        "        self.data = data\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_idx = self.index[index][0]\n",
        "        return self.data[data_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHBhlAejee03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SplitLoader():\n",
        "  def __init__(self, training_dataset, num_workers, batch_size = 100, iid_data = False):\n",
        "    #split the data\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = iid_data\n",
        "    self.num_workers = num_workers\n",
        "    self.full_data = training_dataset\n",
        "    self.size = math.floor(len(self.full_data)/self.num_workers)\n",
        "    #sort the data or shuffle it\n",
        "    self.indices_labels = []\n",
        "    self.sorting()\n",
        "    \n",
        "    self.indices_per_worker = []\n",
        "    for i in range(0, self.num_workers):\n",
        "      self.indices_per_worker.append(self.indices_labels[i*self.size:i*self.size+self.size])\n",
        "    self.setWorkerLoaders()\n",
        "\n",
        "  def setWorkerLoaders(self):\n",
        "    self.full_training_loader = torch.utils.data.DataLoader(self.full_data, 100, shuffle=True)\n",
        "    self.data_loaders = []\n",
        "    for worker in range(0, self.num_workers):\n",
        "      partition = Partition(self.full_data, self.indices_per_worker[worker])\n",
        "      self.data_loaders.append(torch.utils.data.DataLoader(partition, self.batch_size, shuffle=True))\n",
        "\n",
        "  def takeLabel(self, elem):\n",
        "    return elem[1]\n",
        "\n",
        "  def sorting(self):\n",
        "    for index, data in enumerate(self.full_data):\n",
        "      self.indices_labels.append((index, data[1]))  \n",
        "    if self.shuffle:\n",
        "      random.shuffle(self.indices_labels)\n",
        "    else:\n",
        "      self.indices_labels.sort(key=self.takeLabel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFInJ_aOgmsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Configuration():\n",
        "  def __init__(self, dict):\n",
        "    self.dict = dict\n",
        "    self.experiment_id = dict['experiment_id']\n",
        "    self.dataset = dict['dataset']\n",
        "    self.num_workers = dict['num_workers']\n",
        "    self.worker_batch_size = dict['worker_batch_size']\n",
        "    self.validation_batch_size = dict['validation_batch_size']\n",
        "    self.iid_data = dict['iid_data']\n",
        "    self.num_local_steps = dict['num_local_steps']\n",
        "    self.step_size = dict['step_size']\n",
        "    self.sampling_rate = dict['sampling_rate']\n",
        "    self.with_training_loss = dict['with_training_loss']\n",
        "    self.num_workers = dict['num_workers']\n",
        "    self.num_rounds_to_run = dict['num_rounds_to_run']\n",
        "    self.algorithm = dict['algorithm']\n",
        "    self.criterion = dict['criterion']\n",
        "    self.with_training_loss = dict['with_training_loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Yc3XOfelrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runExperiment(validation_loader, splitter, config):          \n",
        "  # Create the logger experiment in wandb\n",
        "  wandb.init(project = config.experiment_id, config = config.dict)\n",
        "\n",
        "  # if config.algorithm == 'DAC':\n",
        "  #   config.step_size /= 1.2 * math.sqrt(config.num_local_steps)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  trainer = Trainer(splitter, validation_loader, config.num_workers)\n",
        "  model = CNN(config.dataset).to(device)\n",
        "\n",
        "  print(\"num_local_steps \", config.num_local_steps, \" num_workers \", config.num_workers, \n",
        "        \" batch_size per worker \", config.worker_batch_size, \" sampling_rate \", config.sampling_rate, ' step_size ', config.step_size)\n",
        "  trainer.setUpExperiment(model, device, config)\n",
        "  # #save data\n",
        "  # wandb.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgXA9UnMfAbO",
        "colab_type": "code",
        "outputId": "f454e6a9-c99f-4fa8-c51b-7f719bc11081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config_dict = {\n",
        "    'dataset':'emnist-byclass',\n",
        "    'experiment_id':'delayed_avg_correction',\n",
        "    'num_workers':698,\n",
        "    'worker_batch_size':25,\n",
        "    'validation_batch_size':100,\n",
        "    'iid_data': False,\n",
        "    'num_local_steps': 1,\n",
        "    'step_size': 0.001,\n",
        "    'sampling_rate': 1.0/35,\n",
        "    'with_training_loss': False,\n",
        "    'num_rounds_to_run': 300,\n",
        "    'algorithm':'FedLearn', #'DAC' or 'FedLearn'\n",
        "    'criterion':nn.CrossEntropyLoss(),\n",
        "    'with_training_loss':False\n",
        "}\n",
        "\n",
        "config = Configuration(config_dict)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, \n",
        "                                                batch_size = config.validation_batch_size, shuffle=False)\n",
        "splitter = SplitLoader(training_dataset, config.num_workers, \n",
        "                       config.worker_batch_size, iid_data = config.iid_data)\n",
        "runExperiment(validation_loader, splitter, config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/lfbarba/delayed_avg_correction\" target=\"_blank\">https://app.wandb.ai/lfbarba/delayed_avg_correction</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/lfbarba/delayed_avg_correction/runs/04wgmiel\" target=\"_blank\">https://app.wandb.ai/lfbarba/delayed_avg_correction/runs/04wgmiel</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "num_local_steps  1  num_workers  698  batch_size per worker  25  sampling_rate  0.02857142857142857  step_size  0.001\n",
            "Num_local_steps :  1 , rounds communication :  2\n",
            "Validation loss: 4.0469, Validation acc 1.9115 \n",
            "Num_local_steps :  1 , rounds communication :  4\n",
            "Validation loss: 3.9286, Validation acc 8.2483 \n",
            "Num_local_steps :  1 , rounds communication :  6\n",
            "Validation loss: 3.8858, Validation acc 5.4055 \n",
            "Num_local_steps :  1 , rounds communication :  8\n",
            "Validation loss: 3.7104, Validation acc 8.0481 \n",
            "Num_local_steps :  1 , rounds communication :  10\n",
            "Validation loss: 3.6207, Validation acc 14.6443 \n",
            "Num_local_steps :  1 , rounds communication :  12\n",
            "Validation loss: 3.3897, Validation acc 21.0146 \n",
            "Num_local_steps :  1 , rounds communication :  14\n",
            "Validation loss: 3.3191, Validation acc 23.7311 \n",
            "Num_local_steps :  1 , rounds communication :  16\n",
            "Validation loss: 3.0546, Validation acc 31.9278 \n",
            "Num_local_steps :  1 , rounds communication :  18\n",
            "Validation loss: 2.9928, Validation acc 36.2070 \n",
            "Num_local_steps :  1 , rounds communication :  20\n",
            "Validation loss: 2.9536, Validation acc 37.7741 \n",
            "Num_local_steps :  1 , rounds communication :  22\n",
            "Validation loss: 2.8319, Validation acc 37.7216 \n",
            "Num_local_steps :  1 , rounds communication :  24\n",
            "Validation loss: 2.7897, Validation acc 39.2337 \n",
            "Num_local_steps :  1 , rounds communication :  26\n",
            "Validation loss: 2.7449, Validation acc 40.0610 \n",
            "Num_local_steps :  1 , rounds communication :  28\n",
            "Validation loss: 2.6303, Validation acc 43.9296 \n",
            "Num_local_steps :  1 , rounds communication :  30\n",
            "Validation loss: 2.6586, Validation acc 45.4407 \n",
            "Num_local_steps :  1 , rounds communication :  32\n",
            "Validation loss: 2.6025, Validation acc 41.4837 \n",
            "Num_local_steps :  1 , rounds communication :  34\n",
            "Validation loss: 2.5367, Validation acc 41.3935 \n",
            "Num_local_steps :  1 , rounds communication :  36\n",
            "Validation loss: 2.4408, Validation acc 44.8411 \n",
            "Num_local_steps :  1 , rounds communication :  38\n",
            "Validation loss: 2.3306, Validation acc 46.5215 \n",
            "Num_local_steps :  1 , rounds communication :  40\n",
            "Validation loss: 2.2542, Validation acc 47.6040 \n",
            "Num_local_steps :  1 , rounds communication :  42\n",
            "Validation loss: 2.1939, Validation acc 49.0129 \n",
            "Num_local_steps :  1 , rounds communication :  44\n",
            "Validation loss: 2.1317, Validation acc 50.1022 \n",
            "Num_local_steps :  1 , rounds communication :  46\n",
            "Validation loss: 2.0859, Validation acc 50.8144 \n",
            "Num_local_steps :  1 , rounds communication :  48\n",
            "Validation loss: 2.0259, Validation acc 52.6598 \n",
            "Num_local_steps :  1 , rounds communication :  50\n",
            "Validation loss: 1.9988, Validation acc 53.7577 \n",
            "Num_local_steps :  1 , rounds communication :  52\n",
            "Validation loss: 1.9724, Validation acc 54.1701 \n",
            "Num_local_steps :  1 , rounds communication :  54\n",
            "Validation loss: 1.9530, Validation acc 54.8883 \n",
            "Num_local_steps :  1 , rounds communication :  56\n",
            "Validation loss: 1.9275, Validation acc 55.9871 \n",
            "Num_local_steps :  1 , rounds communication :  58\n",
            "Validation loss: 1.8640, Validation acc 57.7388 \n",
            "Num_local_steps :  1 , rounds communication :  60\n",
            "Validation loss: 1.8350, Validation acc 57.6564 \n",
            "Num_local_steps :  1 , rounds communication :  62\n",
            "Validation loss: 1.7878, Validation acc 58.5601 \n",
            "Num_local_steps :  1 , rounds communication :  64\n",
            "Validation loss: 1.7495, Validation acc 59.2131 \n",
            "Num_local_steps :  1 , rounds communication :  66\n",
            "Validation loss: 1.7217, Validation acc 59.9141 \n",
            "Num_local_steps :  1 , rounds communication :  68\n",
            "Validation loss: 1.6877, Validation acc 60.4674 \n",
            "Num_local_steps :  1 , rounds communication :  70\n",
            "Validation loss: 1.6493, Validation acc 61.2345 \n",
            "Num_local_steps :  1 , rounds communication :  72\n",
            "Validation loss: 1.6607, Validation acc 61.0833 \n",
            "Num_local_steps :  1 , rounds communication :  74\n",
            "Validation loss: 1.6839, Validation acc 61.2543 \n",
            "Num_local_steps :  1 , rounds communication :  76\n",
            "Validation loss: 1.6567, Validation acc 60.9691 \n",
            "Num_local_steps :  1 , rounds communication :  78\n",
            "Validation loss: 1.6283, Validation acc 60.8754 \n",
            "Num_local_steps :  1 , rounds communication :  80\n",
            "Validation loss: 1.5798, Validation acc 61.9562 \n",
            "Num_local_steps :  1 , rounds communication :  82\n",
            "Validation loss: 1.5479, Validation acc 62.8428 \n",
            "Num_local_steps :  1 , rounds communication :  84\n",
            "Validation loss: 1.4930, Validation acc 64.0902 \n",
            "Num_local_steps :  1 , rounds communication :  86\n",
            "Validation loss: 1.4889, Validation acc 63.7448 \n",
            "Num_local_steps :  1 , rounds communication :  88\n",
            "Validation loss: 1.4739, Validation acc 63.1796 \n",
            "Num_local_steps :  1 , rounds communication :  90\n",
            "Validation loss: 1.5219, Validation acc 61.1942 \n",
            "Num_local_steps :  1 , rounds communication :  92\n",
            "Validation loss: 1.5180, Validation acc 60.8711 \n",
            "Num_local_steps :  1 , rounds communication :  94\n",
            "Validation loss: 1.4585, Validation acc 62.5842 \n",
            "Num_local_steps :  1 , rounds communication :  96\n",
            "Validation loss: 1.4041, Validation acc 64.5498 \n",
            "Num_local_steps :  1 , rounds communication :  98\n",
            "Validation loss: 1.3682, Validation acc 66.2345 \n",
            "Num_local_steps :  1 , rounds communication :  100\n",
            "Validation loss: 1.4024, Validation acc 65.1392 \n",
            "Num_local_steps :  1 , rounds communication :  102\n",
            "Validation loss: 1.3836, Validation acc 65.1426 \n",
            "Num_local_steps :  1 , rounds communication :  104\n",
            "Validation loss: 1.3568, Validation acc 66.0172 \n",
            "Num_local_steps :  1 , rounds communication :  106\n",
            "Validation loss: 1.3274, Validation acc 67.3574 \n",
            "Num_local_steps :  1 , rounds communication :  108\n",
            "Validation loss: 1.3047, Validation acc 67.8746 \n",
            "Num_local_steps :  1 , rounds communication :  110\n",
            "Validation loss: 1.2932, Validation acc 67.7397 \n",
            "Num_local_steps :  1 , rounds communication :  112\n",
            "Validation loss: 1.2813, Validation acc 67.8608 \n",
            "Num_local_steps :  1 , rounds communication :  114\n",
            "Validation loss: 1.2759, Validation acc 66.2277 \n",
            "Num_local_steps :  1 , rounds communication :  116\n",
            "Validation loss: 1.2607, Validation acc 66.1280 \n",
            "Num_local_steps :  1 , rounds communication :  118\n",
            "Validation loss: 1.2587, Validation acc 66.3239 \n",
            "Num_local_steps :  1 , rounds communication :  120\n",
            "Validation loss: 1.2267, Validation acc 68.1993 \n",
            "Num_local_steps :  1 , rounds communication :  122\n",
            "Validation loss: 1.2136, Validation acc 69.0232 \n",
            "Num_local_steps :  1 , rounds communication :  124\n",
            "Validation loss: 1.2393, Validation acc 67.9863 \n",
            "Num_local_steps :  1 , rounds communication :  126\n",
            "Validation loss: 1.2266, Validation acc 67.5893 \n",
            "Num_local_steps :  1 , rounds communication :  128\n",
            "Validation loss: 1.2150, Validation acc 67.7569 \n",
            "Num_local_steps :  1 , rounds communication :  130\n",
            "Validation loss: 1.2263, Validation acc 67.7775 \n",
            "Num_local_steps :  1 , rounds communication :  132\n",
            "Validation loss: 1.2033, Validation acc 68.4768 \n",
            "Num_local_steps :  1 , rounds communication :  134\n",
            "Validation loss: 1.1762, Validation acc 68.1804 \n",
            "Num_local_steps :  1 , rounds communication :  136\n",
            "Validation loss: 1.2167, Validation acc 67.2637 \n",
            "Num_local_steps :  1 , rounds communication :  138\n",
            "Validation loss: 1.2372, Validation acc 67.1804 \n",
            "Num_local_steps :  1 , rounds communication :  140\n",
            "Validation loss: 1.1671, Validation acc 68.1967 \n",
            "Num_local_steps :  1 , rounds communication :  142\n",
            "Validation loss: 1.1446, Validation acc 69.0103 \n",
            "Num_local_steps :  1 , rounds communication :  144\n",
            "Validation loss: 1.1809, Validation acc 68.4759 \n",
            "Num_local_steps :  1 , rounds communication :  146\n",
            "Validation loss: 1.1874, Validation acc 68.7792 \n",
            "Num_local_steps :  1 , rounds communication :  148\n",
            "Validation loss: 1.1493, Validation acc 69.3677 \n",
            "Num_local_steps :  1 , rounds communication :  150\n",
            "Validation loss: 1.1362, Validation acc 69.6555 \n",
            "Num_local_steps :  1 , rounds communication :  152\n",
            "Validation loss: 1.1286, Validation acc 69.2414 \n",
            "Num_local_steps :  1 , rounds communication :  154\n",
            "Validation loss: 1.1163, Validation acc 69.0962 \n",
            "Num_local_steps :  1 , rounds communication :  156\n",
            "Validation loss: 1.1129, Validation acc 69.4759 \n",
            "Num_local_steps :  1 , rounds communication :  158\n",
            "Validation loss: 1.0872, Validation acc 70.3162 \n",
            "Num_local_steps :  1 , rounds communication :  160\n",
            "Validation loss: 1.0519, Validation acc 70.8780 \n",
            "Num_local_steps :  1 , rounds communication :  162\n",
            "Validation loss: 1.0459, Validation acc 69.7380 \n",
            "Num_local_steps :  1 , rounds communication :  164\n",
            "Validation loss: 1.0617, Validation acc 69.1263 \n",
            "Num_local_steps :  1 , rounds communication :  166\n",
            "Validation loss: 1.0199, Validation acc 71.6976 \n",
            "Num_local_steps :  1 , rounds communication :  168\n",
            "Validation loss: 1.0149, Validation acc 71.4347 \n",
            "Num_local_steps :  1 , rounds communication :  170\n",
            "Validation loss: 1.0181, Validation acc 71.2028 \n",
            "Num_local_steps :  1 , rounds communication :  172\n",
            "Validation loss: 1.0261, Validation acc 70.9854 \n",
            "Num_local_steps :  1 , rounds communication :  174\n",
            "Validation loss: 1.0599, Validation acc 70.1005 \n",
            "Num_local_steps :  1 , rounds communication :  176\n",
            "Validation loss: 1.0303, Validation acc 71.2826 \n",
            "Num_local_steps :  1 , rounds communication :  178\n",
            "Validation loss: 1.0082, Validation acc 72.1108 \n",
            "Num_local_steps :  1 , rounds communication :  180\n",
            "Validation loss: 1.0059, Validation acc 71.6796 \n",
            "Num_local_steps :  1 , rounds communication :  182\n",
            "Validation loss: 0.9785, Validation acc 72.1005 \n",
            "Num_local_steps :  1 , rounds communication :  184\n",
            "Validation loss: 0.9652, Validation acc 72.3265 \n",
            "Num_local_steps :  1 , rounds communication :  186\n",
            "Validation loss: 0.9671, Validation acc 71.5515 \n",
            "Num_local_steps :  1 , rounds communication :  188\n",
            "Validation loss: 0.9719, Validation acc 71.6486 \n",
            "Num_local_steps :  1 , rounds communication :  190\n",
            "Validation loss: 0.9741, Validation acc 71.1332 \n",
            "Num_local_steps :  1 , rounds communication :  192\n",
            "Validation loss: 0.9591, Validation acc 71.2208 \n",
            "Num_local_steps :  1 , rounds communication :  194\n",
            "Validation loss: 0.9609, Validation acc 71.9734 \n",
            "Num_local_steps :  1 , rounds communication :  196\n",
            "Validation loss: 1.0085, Validation acc 70.6005 \n",
            "Num_local_steps :  1 , rounds communication :  198\n",
            "Validation loss: 0.9988, Validation acc 70.7680 \n",
            "Num_local_steps :  1 , rounds communication :  200\n",
            "Validation loss: 0.9502, Validation acc 73.0180 \n",
            "Num_local_steps :  1 , rounds communication :  202\n",
            "Validation loss: 0.9635, Validation acc 72.4759 \n",
            "Num_local_steps :  1 , rounds communication :  204\n",
            "Validation loss: 0.9862, Validation acc 71.9141 \n",
            "Num_local_steps :  1 , rounds communication :  206\n",
            "Validation loss: 0.9790, Validation acc 72.2363 \n",
            "Num_local_steps :  1 , rounds communication :  208\n",
            "Validation loss: 0.9387, Validation acc 73.2826 \n",
            "Num_local_steps :  1 , rounds communication :  210\n",
            "Validation loss: 0.8958, Validation acc 74.7869 \n",
            "Num_local_steps :  1 , rounds communication :  212\n",
            "Validation loss: 0.9193, Validation acc 73.5326 \n",
            "Num_local_steps :  1 , rounds communication :  214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMijWfnOx0Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for images, labels in splitter.data_loaders[0]:\n",
        "#   break\n",
        "# fig = plt.figure(figsize=(25, 4))\n",
        "\n",
        "# for idx in np.arange(20):\n",
        "#   ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "#   plt.imshow(im_convert(images[idx]))\n",
        "#   ax.set_title(labels[idx].item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npDqYP7EKb9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgqEroOSOmjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}